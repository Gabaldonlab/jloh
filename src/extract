#!/usr/bin/env python3

"""
###
JLOH - Inferring Loss of Heterozygosity Blocks from Short-read sequencing data

Copyright (C) 2023 Matteo Schiavinato

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
###
"""



from Bio import SeqIO
import argparse as ap
import os
import sys
import pysam
from time import asctime as at
import pybedtools
from pybedtools import BedTool
import multiprocessing
from operator import itemgetter
from statistics import median
import shutil
import pandas as pd
import random 
import string 


ss = sys.exit

# help section
if len(sys.argv) == 1:
    sys.argv.append("--help")

if (sys.argv[1] in ["--help", "-h", "-help", "help", "getopt", "usage"]):
    sys.stderr.write("""

Extract LOH blocks:
(i) from a VCF file, a BAM file, and a reference FASTA genome sequence
(ii) from a set of those obtained by mapping hybrid reads on both parental genomes and 
calling SNPs

Usage:
jloh extract --vcf <VCF> --ref <FASTA> --bam <BAM> [options]

[default mode]
--vcf               Input VCF file                                              [!]
--bam               BAM file used to call the --vcf variants                    [!]
--ref               Reference FASTA genome                                      [!]

[assign blocks]
--assign-blocks     JLOH will use both references to assign blocks to REF/ALT   [off]
--vcfs              VCF files (space-separated) called on parentals             [!]
--bams              BAM files used to call the --vcfs (space-separated)         [!]
--refs              Parental FASTA genomes (space-separated)                    [!]

[variants]
--min-snps-kbp      Min. SNPs/kbp to retain a block (het,homo)                  [5,1]
--filter-mode       "pass" to keep only PASS variants, "all" to keep everything [all]
--min-af            Min. allele frequency to consider a variant heterozygous    [0.3]
--max-af            Max. allele frequency to consider a variant heterozygous    [0.7]

[blocks]
--min-length        Min. LOH block length                                       [100]
--coarseness        Don't consider any interval (het/homo) smaller than this    [100]
--min-frac-cov      Min. fraction of LOH block that has to be covered by reads  [0.5]
--hemi              Frac. of the mean coverage under which LOH is hemizygous    [0.75]
--overhang          Kb up/downstream of the LOH block for coverage comparison   [5000]
--min-overhang      Min frac. (0.0-1.0) of overhang to assign zygosity          [0.9]
--merge-uncov       Merge blocks if separated by up to these many uncov pos     [10]

[I/O/E]
--sample            Sample name / Strain name for output files                  [jloh]
--output-dir        Output directory                                            [jloh_out]
--threads           Number of parallel computing threads                        [4]
--regions           BED file containing regions to keep in the analysis         [off]
                    Can include regions from both --refs
--os-scratch        Use the system's TMPDIR                                     [off] 

""")
    sys.exit(0)



p = ap.ArgumentParser()
# default options
p.add_argument("--vcf")
p.add_argument("--bam")
p.add_argument("--ref")
# block assignment options
p.add_argument("--assign-blocks", action="store_true")
p.add_argument("--vcfs", nargs="*")
p.add_argument("--bams", nargs="*")
p.add_argument("--refs", nargs="*")
# I/O/E
p.add_argument("--sample", default="jloh")
p.add_argument("--output-dir", default="jloh_out")
p.add_argument("--threads", default=4, type=int)
p.add_argument("--regions")
p.add_argument("--os-scratch", action="store_true")
# parameters
p.add_argument("--min-snps-kbp", default="5,1", type=str)
p.add_argument("--filter-mode", choices=["all","pass"], default="all")
p.add_argument("--min-length", default=100, type=int)
p.add_argument("--coarseness", default=100, type=int)
p.add_argument("--min-af", default=0.3, type=float)
p.add_argument("--max-af", default=0.7, type=float)
p.add_argument("--min-frac-cov", default=0.5, type=float)
p.add_argument("--hemi", default=0.75, type=float)
p.add_argument("--overhang", default=5000, type=int)
p.add_argument("--min-overhang", default=0.9, type=float)
p.add_argument("--merge-uncov", default=10, type=int)

args = p.parse_args()


# functions

def run_verifications(args):

    """
    Last update: 30/08/2023 
    """

    if \
    (not args.assign_blocks and not os.path.exists(f"{args.bam}.bai")) or \
    (args.assign_blocks and not all([os.path.exists(f"{bam}.bai") for bam in args.bams])):
            
        sys.stderr.write("\nMissing BAM indexes. Index your BAM files before running jloh extract.\n\n")
        sys.exit()

    else:
        return True


def generate_unique_key(output_dir):

    """
    Last update: 12/04/2023 
    """

    unique_key="BEGIN"
    while (os.path.exists(f"{output_dir}/tmp_{unique_key}") == True) or (unique_key=="BEGIN"):
        unique_key = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for _ in range(5))

    return unique_key     



def check_conditions(args):

    """
    Last update: 07/03/2022
    """

    if ((not args.vcf and not args.ref) and not args.assign_blocks):
        sys.stderr.write("ERROR: when using the default mode you must provide a VCF and a REF\n")
        sys.stderr.write("See --vcf, --ref options\n")
        sys.exit()

    if (args.assign_blocks and (not args.vcfs and not args.refs)):
        sys.stderr.write("ERROR: when using the --assign-blocks mode you must provide a VCF and a REF for both A and B subgenomes\n")
        sys.stderr.write("See --vcfs and --refs options\n")
        sys.exit()


def organize_workspace(args):

    """
    Last update: 18/02/2022
    Creation of folders and workspace where the script has to operate
    Verify presence of minimal set of files to work with
    """

    if os.path.exists(args.output_dir) == False:
        os.makedirs(args.output_dir)
        sys.stderr.write(f"[{at()}] Created output directory {args.output_dir}\n")


def dump_queue(q):

    """
    16/03/2022
    """

    out = []
    while not q.empty():
        x = q.get()
        out.append(x)

    return out


def hetero_and_homo_snps(vcf):

    """
    Last update: 04/03/2022
    """

    # read SNPs
    INPUT = open(vcf, "r")
    Lines = [ line for line in INPUT ]
    Header = [ line for line in Lines if line[0] == "#" ]
    Vcf_lines = [ line for line in Lines if line[0] != "#" ]
    INPUT.close()

    Het_lines, Homo_lines = [],[]

    for line in Vcf_lines:

        # split by field
        lst = line.rstrip("\b\r\n").split("\t")

        # read values
        annotations = lst[8].split(":")
        values = lst[9].split(":")
        dict = { annotations[i]:values[i] for i in range(0, len(annotations)) }

        # Manage phased genotypes
        if len(dict["GT"]) > 1:
            if dict["GT"][1] == "|":
                dict["GT"] = dict["GT"].replace("|", "/")

        # if it's a single heterozygous SNP
        if ((len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="0/1")):

            # 4. write out lines that have fitting values
            Het_lines.append(line)

        # if homozygous: keep for later assignment to blocks
        elif ((len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="0/0") \
        or (len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="1/1")):

            Homo_lines.append(line)

        # consider multiallelic sites
        # only if all alleles are SNPs
        # and if all alleles are not stars (spanning deletions)
        elif ((len(lst[3]) == 1) and (len(lst[4].split(",")) > 1)
        and (all([ len(x)==1 for x in lst[4].split(",") ])) \
        and (all([ x!="*" for x in lst[4].split(",") ]))):

            # this means that there are two annotations for AF
            # and that both have to be checked
            # so it is a variation on the previous block
            # which could be rendered into a function
            # assuming that there are > 1 AF annotation
            # splitting the field based on the comma
            # and converting to float the content
            AFs = [ float(x) for x in dict["AF"].split(",") ]

            # all it takes is one of the variants to be heterozygous
            # for the locus to be conserved
            # where het = AF comprised between --min-af and --max-af
            # 4. write out lines that have fitting values
            Het_lines.append(line)

    return Het_lines, Homo_lines, Header


def parse_chromosomes(ref):

    """
    Last update: 03/03/2022
    """

    Chrom_lengths = {}
    for record in SeqIO.parse(ref, "fasta"):
        id = record.id
        seq = record.seq
        seqlen = len(seq)
        Chrom_lengths[id] = seqlen

    return Chrom_lengths


def make_genome_file(ref, out_file):

    """
    Last update: 07/03/2022
    """

    OUT = open(out_file, "w")
    Chrom_lengths = {}
    for record in SeqIO.parse(ref, "fasta"):
        id = record.id
        seq = record.seq
        seqlen = len(seq)
        OUT.write(f"{id}\t{seqlen}\n")

    OUT.close()

    return True


def create_subset_bam_file(bam, tmp_bams, chrom, queue):

    """
    Last update: 28/09/2022
    """

    if bam.split(".")[-1] == "bam":
        b = pysam.AlignmentFile(bam, "rb")
    elif bam.split(".")[-1] == "sam":
        b = pysam.AlignmentFile(bam, "r")
    else:
        sys.stderr.write(f"\nERROR: could not read bam file:\n{bam}\n\n")

    outfile = f"{tmp_bams}/{chrom}.bam"
    bsub = pysam.AlignmentFile(outfile, "wb", template=b)

    for read in b.fetch(chrom):
        if (read.is_unmapped == False) and (read.is_secondary == False):
            bsub.write(read)

    bsub.close()
    b.close()
    pysam.index(outfile)

    queue.put((chrom, outfile))


def split_bam_by_chromosome(bam, genome_file, tmp_bams, args):

    """
    Last update: 28/09/2022
    """

    Chroms = pd.read_csv(genome_file, sep="\t", header=None)
    Chroms.columns = ["Chrom", "Length"]
    Chroms = Chroms["Chrom"].tolist()

    pool = multiprocessing.Pool(processes=args.threads)
    queue = multiprocessing.Manager().Queue()

    Chrom_bams = {}
    for chrom in Chroms:
        pool.apply_async(create_subset_bam_file, args=(bam, tmp_bams, chrom, queue))

    # terminate pool
    pool.close()
    pool.join()

    # dump queue
    out = dump_queue(queue)
    Chrom_bams = {x[0]:x[1] for x in out}

    return Chrom_bams


def is_a_snp(line):

    """
    Last update: 03/03/2022
    """

    lst = line.rstrip("\r\b\n").split("\t")
    ref, alt = lst[3].split(","), lst[4].split(",")
    if (all([len(x)==1 for x in ref]) and all([len(y)==1 for y in alt])) == True:
        return True
    else:
        return False


def calculate_chrom_snp_densities(Variants, ref):

    """
    Last update: 03/03/2022
    """

    # extract SNPs and chromosome lengths
    # subdivide SNPs by chromosome
    Snps = [ (line.split("\t")[0], line) for line in Variants if is_a_snp(line) == True ]
    Chrom_lengths = parse_chromosomes(ref)
    Snps_by_chrom = { chrom : [] for chrom in Chrom_lengths.keys() }

    for snp in Snps:
        Snps_by_chrom[snp[0]].append(snp[1])

    # calculate densities by chromosome
    Snp_densities = { chrom : len(Snps_by_chrom[chrom]) / (int(Chrom_lengths[chrom]) / 1000) for chrom in Snps_by_chrom.keys() }

    return Snp_densities


def hetero_and_homo_snp_densities(het_snps_vcf, homo_snps_vcf, ref):

    """
    Last update: 15/03/2022
    """

    Hetero_lines = [ line for line in open(het_snps_vcf, "r") if line[0] != "#" ]
    Homo_lines = [ line for line in open(homo_snps_vcf, "r") if line[0] != "#" ]

    # hetero
    Het_snp_densities = calculate_chrom_snp_densities(Hetero_lines, ref)
    Het_snp_densities = [float(x) for x in Het_snp_densities.values() if x > 0]
    if len(Het_snp_densities) > 0:
        hetero_div = round(median(Het_snp_densities), 2)
    else:
        hetero_div = round(0,2)

    # homo
    Homo_snp_densities = calculate_chrom_snp_densities(Homo_lines, ref)
    Homo_snp_densities = [float(x) for x in Homo_snp_densities.values() if x > 0]
    if len(Homo_snp_densities) > 0:
        homo_div = round(median(Homo_snp_densities), 2)
    else:
        homo_div = round(0,2)

    return hetero_div, homo_div


def filter_by_snp_count(args, bt, discard, min_snps_kbp):

    """
    v1
    Author: Leszek Pryszcz
    Last update: 14/02/2014
    ###
    v2
    Author: Matteo Schiavinato
    Last update: 17/08/2022
    """

    # if False, then these are the SNPs we keep
    if discard == False:
        out = []
        skipped = 0
        for row in bt:
            ref,start,end,count = str(row).rstrip("\n\r\b").split("\t")
            ref,start,end,count = str(ref),int(start),int(end),int(count)
            blocklen = float(int(end)-int(start))
            blocklen_kb = blocklen / 1000
            block_snps_kb = float(count) / blocklen_kb

            if (block_snps_kb >= min_snps_kbp) and (blocklen > 0):
                out.append(row)
            else:
                skipped += 1

        new_bt = BedTool(out)

    # if True, then these are the SNPs we discard
    elif discard == True:
        out = []
        skipped = 0
        for row in bt:

            ref,start,end,count = str(row).rstrip("\n\r\b").split("\t")
            ref,start,end,count = str(ref),int(start),int(end),int(count)
            blocklen = float(int(end)-int(start))
            blocklen_kb = blocklen / 1000
            block_snps_kb = float(count) / blocklen_kb

            if (block_snps_kb < min_snps_kbp) and (blocklen > 0):
                out.append(row)
            else:
                skipped += 1

        new_bt = BedTool(out)

    return(new_bt, skipped)


def filter_by_length(bt, min_length):

    """
    Last update: 28/09/2022
    Note: Keep only candidate regions of a certain minimum size
    """

    lst = [ str(row).rstrip("\b\r\n").split("\t") for row in bt ]
    lst = [ x for x in lst if int(x[2]) > int(x[1]) ]
    lst = [ x for x in lst if int(x[2]) - int(x[1]) >= min_length ]
    new_bt = BedTool(lst)

    return new_bt


def get_snp_intervals(args, in_vcf, invert, min_snps_kbp, snp_distance):

    """
    Last update: 28/09/2022
    """

    # -------------------------------------------
    # Get blocks with SNPs with bedtools merge
    # merge bed file
    # then filter it by SNP distance and count

    # when false, retain only intervals with AT LEAST this number of SNPs
    if invert == False:
        snp_blocks = BedTool(in_vcf)
        if len(snp_blocks) > 0:
            snp_blocks = snp_blocks.merge(d=snp_distance, c=1, o="count")
            (snp_blocks, skipped) = filter_by_snp_count(args, snp_blocks, False, min_snps_kbp)
        else:
            (snp_blocks, skipped) = [], 0

    # when true, retain only intervals with AT MOST this number of SNPs
    elif invert == True:
        snp_blocks = BedTool(in_vcf)
        if len(snp_blocks) > 0:
            snp_blocks = snp_blocks.merge(d=snp_distance, c=1, o="count")
            (snp_blocks, skipped) = filter_by_snp_count(args, snp_blocks, True, min_snps_kbp)
        else:
            (snp_blocks, skipped) = [], 0

    return snp_blocks


def get_homo_ref_blocks(Het, Homo_ALT, genome_file):

    """
    Last update: 28/09/2022
    """

    HET = [str(row).rstrip("\n\r\b").split("\t") for row in Het]
    ALT = [str(row).rstrip("\n\r\b").split("\t") for row in Homo_ALT]
    HET = [(str(x[0]), int(x[1]), int(x[2]), int(x[3])) for x in HET]
    ALT = [(str(x[0]), int(x[1]), int(x[2]), int(x[3])) for x in ALT]
    HET = sorted(HET, key=itemgetter(0,1,2))
    ALT = sorted(ALT, key=itemgetter(0,1,2))
    HET_ALT = HET + ALT
    HET_ALT = sorted(HET_ALT, key=itemgetter(0,1,2))
    HET_ALT = [[str(i) for i in x] for x in HET_ALT]
    HET_ALT = ["\t".join(x)+"\n" for x in HET_ALT]
    bt_het_alt = BedTool(HET_ALT).sort(g=genome_file)
    bt_het_alt = bt_het_alt.merge()
    bt_homo_REF = bt_het_alt.complement(g=genome_file, L=True)

    return bt_homo_REF


def snps_to_bed_blocks(args, het_snps_vcf, homo_snps_vcf, genome_file, min_snps_kbp):

    """
    Last update: 23/01/2023
    """

    # define min snps and snp dist values
    min_snps_het = max(float(min_snps_kbp.split(",")[0]), float(1))
    min_snps_homo = max(float(min_snps_kbp.split(",")[1]), float(1))
    snp_dist_het = int(round(float(1000)/min_snps_het, 0))
    snp_dist_homo = int(round(float(1000)/min_snps_homo, 0))
    Min_snps = {"het":min_snps_het, "homo":min_snps_homo}
    Snp_dist = {"het":snp_dist_het, "homo":snp_dist_homo}

    pool = multiprocessing.Pool(args.threads)

    Het_bed_blocks = pool.apply_async(get_snp_intervals, args=(args, het_snps_vcf, False, Min_snps["het"], Snp_dist["het"]))
    Homo_bed_blocks_ALT = pool.apply_async(get_snp_intervals, args=(args, homo_snps_vcf, False, Min_snps["homo"], Snp_dist["homo"]))

    pool.close()
    pool.join()

    Het_bed_blocks = Het_bed_blocks.get()

    # combine together het blocks that are nearby
    merge_len=args.min_length-1
    Het_bed_blocks = BedTool(Het_bed_blocks).merge(d=merge_len, c=4, o="sum")

    Homo_bed_blocks_ALT = Homo_bed_blocks_ALT.get()

    Homo_bed_blocks_REF = get_homo_ref_blocks(Het_bed_blocks, Homo_bed_blocks_ALT, genome_file)
    Homo_bed_blocks_REF = Homo_bed_blocks_REF.merge(d=merge_len)
    Homo_bed_blocks_REF = count_snps_in_bed_interval(Homo_bed_blocks_REF, homo_snps_vcf)

    return Het_bed_blocks, Homo_bed_blocks_REF, Homo_bed_blocks_ALT


def remove_overlapping_regions(bt_A, bt_B, args):

    """
    Last update: 28/09/2022
    """

    if ((len(bt_A) > 0) and (len(bt_B) > 0)):
        # remove small intervals
        bt_A = BedTool([row for row in bt_A if int(row[2])-int(row[1])>=args.coarseness])
        bt_B = BedTool([row for row in bt_B if int(row[2])-int(row[1])>=args.coarseness])
        # remove intersection between the two beds
        bt = bt_A.subtract(b=bt_B)
        # remove short ones
        lst = BedTool([row for row in bt if (int(row[2]) > int(row[1])) and (int(row[2])-int(row[1])>=args.coarseness)])
        # sort
        bt = bt.sort()

        # write to output
        return bt

    else:
        return []



def adjust_homo_coordinates_by_hetero_overlap(args, Het_blocks, Homo_blocks_REF, Homo_blocks_ALT):

    """
    Last update: 27/09/2022
    """

    pool = multiprocessing.Pool(processes=args.threads)

    Homo_blocks_REF = pool.apply_async(remove_overlapping_regions, args=(Homo_blocks_REF,
                                                                        Het_blocks, args))

    Homo_blocks_ALT = pool.apply_async(remove_overlapping_regions, args=(Homo_blocks_ALT,
                                                                        Het_blocks, args))

    pool.close()
    pool.join()

    Homo_blocks_ALT = Homo_blocks_ALT.get()
    Homo_blocks_REF = Homo_blocks_REF.get()

    Homo_REF = add_column(Homo_blocks_REF, "REF")
    Homo_ALT = add_column(Homo_blocks_ALT, "ALT")
    Homo_blocks = combine_beds(Homo_REF, Homo_ALT)

    return Homo_blocks


def add_column(bt, annot):

    """
    Last update: 28/09/2022
    Note: Add the "annot" value as extra column to a BED file
    """

    x = [ str(row).rstrip("\b\r\n") + "\t" + str(annot) for row in bt ]
    new_bt = BedTool(x)

    return new_bt


def combine_beds(bt_A, bt_B):

    """
    Last update: 28/09/2022
    Note: combine two bed files into one
    """

    x = list(bt_A)
    y = list(bt_B)
    z = x + y
    bt = BedTool(z).sort()

    return bt


def get_chromosomal_covered_regions(queue, chrom_bam, chrom, chrom_len):

    """
    Last update: 28/09/2022
    """

    # read bam file and convert into bed file
    # index bam file if needed
    if os.path.exists(f"{bam}.bai") == False:
        res = index_bam(bam)
        if res == False:
            sys.exit("ERROR: The BAM file could not be indexed.\n\n")

    bt = BedTool(chrom_bam)
    bt = bt.bamtobed()

    # merge overlapping intervals
    bt = bt.merge(bed=True)

    # convert to list
    bt = [ str(row) for row in bt ]

    # remove too-short ones
    bt = [x for x in bt if int(x.split("\t")[2]) > int(x.split("\t")[1])]

    # put in queue
    queue.put(bt)



def remove_negative_intervals(args, bt):

    """
    28/09/2022
    This function only bypasses an issue of bedtools that they haven't fixed yet
    This bug creates an interval going from 1 to -1 which then is impossible to parse
    from pybedtools
    Open github issue:
    https://github.com/arq5x/bedtools2/issues/979
    """

    tmp_out = f"{args.output_dir}/TMP.internal_service_file.bed"
    bt.saveas(tmp_out)
    Bed_lines = [ line.rstrip("\r\b\n").split("\t") for line in open(tmp_out, "r") ]
    Bed_lines = [ (str(x[0]), int(x[1]), int(x[2])) for x in Bed_lines ]
    Bed_lines = [ x for x in Bed_lines if ((x[1] < x[2]) and (x[2] > 0)) ]
    bt = BedTool(Bed_lines)
    os.remove(tmp_out)

    return bt


def fix_coordinates_by_cov(Homo_blocks, bt_uncov_regions):

    """
    Last update: 28/09/2022
    """

    # intersect the two bed files (blocks and uncovered regions)
    # adjust coordinates
    bt_blocks = BedTool(Homo_blocks)
    bt_uncov = BedTool(bt_uncov_regions)
    bt_blocks_adj = bt_blocks.subtract(b=bt_uncov)

    # remove short ones
    adj_blocks = [ str(x).rstrip("\n\r\b").split("\t") for x in bt_blocks_adj ]
    adj_blocks = [ x for x in adj_blocks if (int(x[1]) >= 0 and int(x[2]) > int(x[1])) ]
    bt_blocks_adj = BedTool(adj_blocks)

    # sort
    bt_blocks_adj = bt_blocks_adj.sort()

    return bt_blocks_adj


def remove_uncovered_regions(args, Chrom_bams, ref, Homo_blocks, genome_file, tmpdir):

    """
    Last update: 28/09/2022
    """

    pool = multiprocessing.Pool(processes=args.threads)
    queue = multiprocessing.Manager().Queue()

    Chrom_lengths = parse_chromosomes(ref)

    # run function
    for chrom in Chrom_lengths.keys():
        pool.apply_async(get_chromosomal_covered_regions, args=(queue, Chrom_bams[chrom], chrom, Chrom_lengths[chrom]))

    # terminate pool
    pool.close()
    pool.join()

    # dump queue
    out = dump_queue(queue)

    # flatten the list
    cov_regions_bed = [ entry.rstrip("\b\r\n").split("\t") for sublst in out for entry in sublst ]

    # convert to bed tool
    bt = BedTool(cov_regions_bed).sort(faidx=genome_file).merge(d=args.merge_uncov)
    bt = bt.sort(faidx=genome_file)

    # invert interval
    bt_complement = bt.complement(g=genome_file, L=True)

    # write to file and read again
    # to bypass the bedtools bug which they won't fix any time soon
    # which creates an interval from 1 to -1 and then makes the whole thing crash
    bt_uncov_regions = remove_negative_intervals(args, bt)

    # fix coordinates
    Homo_blocks_adj = fix_coordinates_by_cov(Homo_blocks, bt_uncov_regions)

    return Homo_blocks_adj


def index_bam(bam):

    """
    v1
    Last update: 03/03/2022
    """

    if os.path.exists(f"{bam}.bai") == False:
        try:
            pysam.index(bam)
            return True
        except:
            return False

    else:
        return True



def get_total_cov_pos(bam, chrom, start, end):

    """
    v1
    Last update: 03/03/2022
    """

    if start < 0:
        start = 0

    # open reader
    # select reads within the region and crop at the end (truncate)
    py_bam = pysam.AlignmentFile(bam)
    pileup = py_bam.pileup(chrom, start, end, truncate=True)
    # get total covered positions using .n which is the number of reads in the "column"
    tot_cov_pos = len([i for i in pileup if int(i.n) > 0])

    return tot_cov_pos


def calculate_coverage_of_a_chromosome(chrom, chrom_bam, chrom_len, queue):

    """
    v1
    Last update: 28/09/2022
    """

    start = 0
    end = chrom_len
    tot_cov_pos = get_total_cov_pos(chrom_bam, chrom, start, end)

    py_bam = pysam.AlignmentFile(chrom_bam)
    pileup = py_bam.pileup(chrom, start, end, truncate=True)

    try:
        avg_cov = round(float(sum([int(i.n) for i in pileup if int(i.n) > 0])) / float(tot_cov_pos), 1)
    except ZeroDivisionError:
        avg_cov = round(float(0), 1)

    queue.put((chrom, avg_cov))


def get_coverage(bam, chrom, start, end):

    """
    v1
    Last update: 03/03/2022
    """

    # get total covered positions
    tot_cov_pos = get_total_cov_pos(bam, chrom, start, end)

    try:
        py_bam = pysam.AlignmentFile(bam)
        pileup = py_bam.pileup(chrom, start, end, truncate=True)
        avg_cov = round(float(sum([int(i.n) for i in pileup if int(i.n) > 0])) / float(tot_cov_pos), 1)
    except ZeroDivisionError:
        avg_cov = round(float(0), 1)

    return avg_cov


def get_zygosity(block_cov, bam, chrom, start, end, overhang, min_overhang, hemi_threshold, chrom_len):

    """
    v1
    Last update: 07/03/2022
    """

    # define up/down coordinates
    up_start = max(start-overhang, 0)
    up_end = start
    up_length = up_end - up_start
    down_start = end
    down_end = min(end+overhang, chrom_len)
    down_length = down_end - down_start

    # get cov ratio with 5 kb upstream
    # avoiding negative coordinates
    up_cov = get_coverage(bam, chrom, up_start, start)
    # get cov ratio with 5 kb downstream
    # avoiding positions outside of the chrom max len in 0-based format
    down_cov = get_coverage(bam, chrom, end, down_end)

    # compare with block coverage
    if up_length >= min_overhang * overhang:
        try:
            # up_ratio = min(block_cov / up_cov * 100, 100)
            up_ratio = block_cov / up_cov
        except ZeroDivisionError:
            up_ratio = 100
    else:
        up_ratio = None

    # down block
    if down_length >= 0.9 * overhang:
        try:
            # down_ratio = min(block_cov / down_cov * 100, 100)
            down_ratio = block_cov / down_cov
        except ZeroDivisionError:
            down_ratio = 100
    else:
        down_ratio = None

    # proceed with zygosity only if overhangs up/downstream are 90% of the chosen length at least
    # if not, write "NA"
    if all([x != None for x in [up_ratio, down_ratio]]):

        # if ratios are < --hemi,  block is hemizygous
        # if ratios are >= --hemi, block is homozygous
        if (up_ratio < hemi_threshold) and (down_ratio < hemi_threshold):
            zygosity = "hemi"
        elif (up_ratio >= hemi_threshold) and (down_ratio >= hemi_threshold):
            zygosity = "homo"
        else:
            zygosity = "NA"

    # if flanking regions are too small write "NA" directly (can't trust it)
    else:
        zygosity = "NA"

    return zygosity, up_ratio, down_ratio



def process_region_coverage(queue, row, chrom_bam, min_frac_cov, hemi_threshold, chrom_meancov, overhang, min_overhang, chrom_len):

    """
    v1
    Author: Leszek Pryszcz
    Last update: 2014
    ###
    v2
    Author: Matteo Schiavinato
    Last Update: 28/09/2022
    """

    bed = str(row)

    # unload bed coordinate
    chrom, start, end, snps, block_type = bed.rstrip("\n\b\r").split('\t')

    start, end = int(start), int(end)
    length = end-start

    # get total covererd positions
    tot_cov_pos = get_total_cov_pos(chrom_bam, chrom, start, end)
    # get covered fraction by dividing the two values
    tot_pos = end-start
    cov_frac = round(float(tot_cov_pos) / float(tot_pos), 3)

    # debug:
    if cov_frac > 1.0:
        sys.stderr.write(f"\nERROR: the detected covered fraction ({cov_frac}) in {chrom}, {start}, {end} (0-based) is larger than the region size ({end-start} bp).\n")
        sys.stderr.write("Report this bug in a GitHub issue: it shouldn't happen!\n\n")
        sys.exit()

    if cov_frac >= min_frac_cov:

        # get coverage
        avg_cov = get_coverage(chrom_bam, chrom, start, end)

        # get zygosity
        zygosity, up_ratio, down_ratio = get_zygosity(avg_cov, chrom_bam, chrom, start, end, overhang, min_overhang, hemi_threshold, chrom_len)
        if up_ratio != None:
            up_ratio = round(up_ratio, 2)
        if down_ratio != None:
            down_ratio = round(down_ratio, 2)

        bed_line = f"{chrom}\t{start}\t{end}\t{avg_cov}x\t{cov_frac}\t{up_ratio}\t{down_ratio}\t{zygosity}\t{length}\t{block_type}\n"

    else:
        bed_line = None

    queue.put(bed_line)



def assess_coverage(args, Chrom_bams, ref, bt, handle):

    """
    v1
    Author: Leszek Pryszcz (2014), Veronica Mixao (2019)
    ###
    v2
    Author: Matteo Schiavinato
    Last change: 23/10/2023 
    """

    # get coverage of each chromosome independently
    pool = multiprocessing.Pool(processes=args.threads)
    queue = multiprocessing.Manager().Queue()

    Chrom_lengths = parse_chromosomes(ref)

    for chrom in Chrom_lengths.keys():
        pool.apply_async(calculate_coverage_of_a_chromosome, args=(chrom, Chrom_bams[chrom], Chrom_lengths[chrom], queue))

    pool.close()
    pool.join()

    Out = dump_queue(queue)
    Chrom_covs = {x[0]:x[1] for x in Out}

    # write chromosome coverages to a file
    OUT = open(f"{args.output_dir}/{args.sample}.{handle}.chrom_coverage.tsv", "w")
    for chr in Chrom_covs:
        OUT.write(f"{chr}\t{Chrom_covs[chr]}\n")
    OUT.close()

    # calculate coverage in each block
    pool = multiprocessing.Pool(processes=args.threads)
    queue = multiprocessing.Manager().Queue()

    for row in bt:
        chrom = str(row[0])
        pool.apply_async(process_region_coverage, args=(queue,
                                                        row, Chrom_bams[chrom], args.min_frac_cov,
                                                        args.hemi, Chrom_covs[chrom],
                                                        args.overhang, args.min_overhang,
                                                        Chrom_lengths[chrom]))

    pool.close()
    pool.join()

    Out = dump_queue(queue)

    Out = [ i for i in Out if i != None ]

    bt = BedTool(Out)
    return bt


def count_snps_in_bed_interval(bed, vcf):

    """
    Last update: 21/02/2022
    Note: Count how many SNPs are found inside a BED interval
    by intersecting a BED file with a VCF file
    """

    bed = BedTool(bed)
    vcf = BedTool(vcf)
    bt_inter = bed.intersect(vcf, c=True)
    bt_inter = [ list(x) for x in bt_inter ]
    bt_inter = [ [x[0]] + [int(x[1])] + [int(x[2])] + x[3:] for x in bt_inter ]
    bt_inter_sorted = sorted(bt_inter, key=itemgetter(0,1,2))
    bt_inter_sorted = BedTool(bt_inter_sorted)

    return bt_inter_sorted


def keep_relevant_blocks(args, LOH):

    """
    Last update: 14/03/2022
    """

    bt_regions = BedTool(args.regions)
    bt = BedTool(LOH)

    bt_inter = bt.intersect(bt_regions, u=True)

    return bt_inter


def sort_bed_intervals(bt):

    """
    Last update: 31/03/2022
    """

    bt = BedTool(bt)
    bt = [str(row).rstrip("\r\n\b").split("\t") for row in bt]
    bt_sort = []
    for x in bt:
        new_x = x
        new_x[1]=int(new_x[1])
        new_x[2]=int(new_x[2])
        bt_sort.append(new_x)

    bt_sort = sorted(bt_sort, key=itemgetter(0,1,2))
    bt_sort = BedTool(bt_sort)

    return bt_sort


def assign_empty_allele_annotation(lst):

    """
    Last update: 17/04/2023 
    """

    newlst = []
    for x in lst:
        x[9] = "-"
        newlst.append(x)

    return newlst 
    


def write_vcfs_to_output(args, Handles, mode, Hetero_lines_lst, Homo_lines_lst, Headers):

    """
    Last update: 04/03/2022
    """

    if mode == "default":

        handle = Handles[0]
        Header = Headers[0]
        Hetero_lines = Hetero_lines_lst[0]
        Homo_lines = Homo_lines_lst[0]

        out_hetero_vcf = f"{args.output_dir}/{args.sample}.{handle}.het_snps.vcf"
        out_homo_vcf = f"{args.output_dir}/{args.sample}.{handle}.homo_snps.vcf"

        OUT_HET = open(out_hetero_vcf, "w")
        OUT_HOMO = open(out_homo_vcf, "w")

        for line in Header:
            OUT_HET.write(line)
            OUT_HOMO.write(line)
        for line in Hetero_lines:
            OUT_HET.write(line)
        for line in Homo_lines:
            OUT_HOMO.write(line)

        OUT_HET.close()
        OUT_HOMO.close()

        return { "hetero":[out_hetero_vcf],
                 "homo":[out_homo_vcf] }

    elif mode == "assign_blocks":

        handle_A, handle_B = Handles[0], Handles[1]
        Header_A, Header_B = Headers[0], Headers[1]
        Hetero_lines_A, Hetero_lines_B = Hetero_lines_lst[0], Hetero_lines_lst[1]
        Homo_lines_A, Homo_lines_B = Homo_lines_lst[0], Homo_lines_lst[1]

        out_hetero_vcf_A = f"{args.output_dir}/{args.sample}.{handle_A}.het_snps.vcf"
        out_hetero_vcf_B = f"{args.output_dir}/{args.sample}.{handle_B}.het_snps.vcf"
        out_homo_vcf_A = f"{args.output_dir}/{args.sample}.{handle_A}.homo_snps.vcf"
        out_homo_vcf_B = f"{args.output_dir}/{args.sample}.{handle_B}.homo_snps.vcf"

        OUT_HET_A = open(out_hetero_vcf_A, "w")
        OUT_HET_B = open(out_hetero_vcf_B, "w")
        OUT_HOMO_A = open(out_homo_vcf_A, "w")
        OUT_HOMO_B = open(out_homo_vcf_B, "w")

        for line in Header_A:
            OUT_HET_A.write(line)
            OUT_HOMO_A.write(line)
        for line in Header_B:
            OUT_HET_B.write(line)
            OUT_HOMO_B.write(line)
        for line in Hetero_lines_A:
            OUT_HET_A.write(line)
        for line in Hetero_lines_B:
            OUT_HET_B.write(line)
        for line in Homo_lines_A:
            OUT_HOMO_A.write(line)
        for line in Homo_lines_B:
            OUT_HOMO_B.write(line)

        OUT_HET_A.close()
        OUT_HOMO_A.close()
        OUT_HET_B.close()
        OUT_HOMO_B.close()

        return { "hetero":[out_hetero_vcf_A, out_hetero_vcf_B],
                 "homo":[out_homo_vcf_A, out_homo_vcf_B] }

    else:
        sys.stderr.write("DEBUG: Error in writing VCF files\n")
        sys.exit()


def run_in_default_mode(args, tmp_bams):

    """
    Last update: 30/08/2023
    """

    handle = "exp"

    # verifications 
    tmp = run_verifications(args)

    # snps
    sys.stderr.write(f"[{at()}] Extracting heterozygous and homozygous SNPs...\n")
    Hetero_lines, Homo_lines, Header = hetero_and_homo_snps(args.vcf)
    Vcfs = write_vcfs_to_output(args, [handle], "default", [Hetero_lines], [Homo_lines], [Header])
    sys.stderr.write(f"[{at()}] Found {len(Hetero_lines)} heterozygous SNPs and {len(Homo_lines)} homozygous SNPs\n")

    # divergence
    sys.stderr.write(f"[{at()}] Getting hetero- and homozygous SNP density...\n")
    hetero_div, homo_div = hetero_and_homo_snp_densities(Vcfs["hetero"][0], Vcfs["homo"][0], args.ref)
    sys.stderr.write(f"[{at()}] Avg. homozygous SNPs/kbp: {homo_div}\n")
    sys.stderr.write(f"[{at()}] Avg. heterozygous SNPs/kbp: {hetero_div}\n")

    # genome file
    sys.stderr.write(f"[{at()}] Creating a file with chromosome lengths...\n")
    genome_file = f"{args.output_dir}/{args.sample}.{handle}.genome_file.tsv"
    tmp = make_genome_file(args.ref, genome_file)
    sys.stderr.write(f"[{at()}] Done\n")

    # splitting bam file by chromosome
    sys.stderr.write(f"[{at()}] Creating temporary bam files by chromosome ...\r")
    Chrom_bams = split_bam_by_chromosome(args.bam, genome_file, tmp_bams, args)
    sys.stderr.write("\n")
    sys.stderr.write(f"[{at()}] Done\n")

    # bed blocks
    sys.stderr.write(f"[{at()}] Clustering heterozygous and homozygous SNPs into blocks...\n")
    Het_blocks, Homo_blocks_REF, Homo_blocks_ALT = snps_to_bed_blocks(args, Vcfs["hetero"][0], Vcfs["homo"][0], genome_file, args.min_snps_kbp)
    Het_blocks = filter_by_length(Het_blocks, args.min_length)
    out = f"{args.output_dir}/{args.sample}.{handle}.het_blocks.bed"
    Het_blocks.saveas(out)
    sys.stderr.write(f"[{at()}] Found {len(Het_blocks)} het blocks, {len(Homo_blocks_REF)} homo REF blocks, {len(Homo_blocks_ALT)} homo ALT blocks\n")

    # remove overlaps with heteroblocks from homoblocks
    sys.stderr.write(f"[{at()}] Trimming overlaps of homozygous blocks with any heterozygous block...\n")
    Homo_blocks = adjust_homo_coordinates_by_hetero_overlap(args, Het_blocks, Homo_blocks_REF, Homo_blocks_ALT)
    sys.stderr.write(f"[{at()}] {len(Homo_blocks)} blocks were trimmed\n")

    # remove uncovered regions
    sys.stderr.write(f"[{at()}] Trimming overlaps of homo-blocks with uncovered regions...\n")
    Homo_blocks_adj = remove_uncovered_regions(args, Chrom_bams, args.ref, Homo_blocks, genome_file, tmpdir)
    sys.stderr.write(f"[{at()}] {len(Homo_blocks_adj)} blocks were trimmed\n")

    # analyse block coverage
    sys.stderr.write(f"[{at()}] Computing coverage up- , in-, and downstream of any candidate block...\n")
    LOH = assess_coverage(args, Chrom_bams, args.ref, Homo_blocks, handle)
    sys.stderr.write(f"[{at()}] {len(LOH)} blocks were processed\n")

    # filter by length
    sys.stderr.write(f"[{at()}] Filtering candidate blocks by length...\n")
    LOH = filter_by_length(LOH, args.min_length)
    sys.stderr.write(f"[{at()}] {len(LOH)} blocks survived the filtering\n")

    # count homozygous SNPs in intervals
    sys.stderr.write(f"[{at()}] Adding heterozygous and homozygous SNP counts in each block...\n")
    LOH = count_snps_in_bed_interval(LOH, Vcfs["homo"][0])
    # count heterozygous SNPs in intervals
    LOH = count_snps_in_bed_interval(LOH, Vcfs["hetero"][0])
    sys.stderr.write(f"[{at()}] {len(LOH)} blocks were processed\n")

    # sort
    LOH = sort_bed_intervals(LOH)
    
    # write candidates to output, pre filtering
    out = open(f"{args.output_dir}/{args.sample}.LOH_candidates.tsv", "w")
    out.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")

    for row in LOH:
        item = str(row).rstrip("\b\n\r").split("\t")
        item[1] = str(int(item[1])+1)
        out.write("\t".join(item) + "\n")

    out.close()

    # write candidates to output, pre filtering, in BED format
    LOH_candidates = BedTool([str(row).rstrip("\b\n\r").split("\t")[0:3] for row in LOH])
    LOH_candidates.saveas(f"{args.output_dir}/{args.sample}.LOH_candidates.bed")

    # keep only intervals inside --regions
    if args.regions:
        sys.stderr.write(f"[{at()}] Removing intervals outside of --regions...\n")
        LOH = keep_relevant_blocks(args, LOH)

    sys.stderr.write(f"[{at()}] The final set consists of {len(LOH)} blocks\n")

    # sort intervals
    LOH = sort_bed_intervals(LOH)

    # write to output
    # including a header
    sys.stderr.write(f"[{at()}] Writing to output...\n")
    out = f"{args.output_dir}/{args.sample}.LOH_blocks.tsv"
    out_bed = f"{args.output_dir}/{args.sample}.LOH_blocks.bed"
    OUTPUT = open(out, "w")
    OUTPUT_BED = open(out_bed, "w")

    OUTPUT.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")

    for row in LOH:
        row_1based = str(row).rstrip("\b\n\r").split("\t")
        row_1based[1] = int(row_1based[1])+1
        row_1based = "\t".join([str(i) for i in row_1based]) + "\n"
        OUTPUT.write(str(row_1based))
        row = list(row)[0:3]
        OUTPUT_BED.write("\t".join(row) + "\n")
    OUTPUT.close()
    OUTPUT_BED.close()

    sys.stderr.write(f"[{at()}] Done!\n")


def run_in_block_assignment_mode(args, tmp_bams):

    """
    Last update: 30/08/2023
    """

    handle_A = "exp_A"
    handle_B = "exp_B"

    # verifications 
    tmp = run_verifications(args)

    # snps
    sys.stderr.write(f"[{at()}] Extracting heterozygous and homozygous SNPs...\n")
    Hetero_lines_A, Homo_lines_A, Header_A = hetero_and_homo_snps(args.vcfs[0])
    Hetero_lines_B, Homo_lines_B, Header_B = hetero_and_homo_snps(args.vcfs[1])
    Vcfs = write_vcfs_to_output(args, [handle_A, handle_B], "assign_blocks", [Hetero_lines_A, Hetero_lines_B], [Homo_lines_A, Homo_lines_B], [Header_A, Header_B])
    sys.stderr.write(f"[{at()}] Parent A: Found {len(Hetero_lines_A)} heterozygous SNPs and {len(Homo_lines_A)} homozygous SNPs\n")
    sys.stderr.write(f"[{at()}] Parent B: Found {len(Hetero_lines_B)} heterozygous SNPs and {len(Homo_lines_B)} homozygous SNPs\n")

    # divergence
    sys.stderr.write(f"[{at()}] Getting hetero- and homozygous SNP density in both parental genomes...\n")
    hetero_div_A, homo_div_A = hetero_and_homo_snp_densities(Vcfs["hetero"][0], Vcfs["homo"][0], args.refs[0])
    hetero_div_B, homo_div_B = hetero_and_homo_snp_densities(Vcfs["hetero"][1], Vcfs["homo"][1], args.refs[1])
    sys.stderr.write(f"[{at()}] Parent A: Avg. homo. SNPs/kbp: {homo_div_A}, avg. het. SNPs/kbp: {hetero_div_A}\n")
    sys.stderr.write(f"[{at()}] Parent B: Avg. homo. SNPs/kbp: {homo_div_B}, avg. het. SNPs/kbp: {hetero_div_B}\n")

    # genome file
    sys.stderr.write(f"[{at()}] Creating files with chromosome lengths for all genomes...\n")
    genome_file_A = f"{args.output_dir}/{args.sample}.{handle_A}.genome_file.tsv"
    genome_file_B = f"{args.output_dir}/{args.sample}.{handle_B}.genome_file.tsv"
    tmp = make_genome_file(args.refs[0], genome_file_A)
    tmp = make_genome_file(args.refs[1], genome_file_B)
    sys.stderr.write(f"[{at()}] Done\n")

    # splitting bam file by chromosome
    sys.stderr.write(f"[{at()}] Working on parent A BAM file\n")
    sys.stderr.write(f"[{at()}] Creating temporary bam files by chromosome ...\n")
    Chrom_bams_A = split_bam_by_chromosome(args.bams[0], genome_file_A, tmp_bams, args)

    sys.stderr.write(f"[{at()}] Working on parent B BAM file\n")
    sys.stderr.write(f"[{at()}] Creating temporary bam files by chromosome ...\n")
    Chrom_bams_B = split_bam_by_chromosome(args.bams[1], genome_file_B, tmp_bams, args)
    sys.stderr.write(f"[{at()}] Done\n")

    # bed blocks
    sys.stderr.write(f"[{at()}] Clustering heterozygous and homozygous SNPs into blocks...\n")
    Het_blocks_A, Homo_blocks_REF_A, Homo_blocks_ALT_A = snps_to_bed_blocks(args, Vcfs["hetero"][0], Vcfs["homo"][0], genome_file_A, args.min_snps_kbp)
    Het_blocks_B, Homo_blocks_REF_B, Homo_blocks_ALT_B = snps_to_bed_blocks(args, Vcfs["hetero"][1], Vcfs["homo"][1], genome_file_B, args.min_snps_kbp)
    Het_blocks_A = filter_by_length(Het_blocks_A, args.min_length)
    Het_blocks_B = filter_by_length(Het_blocks_B, args.min_length)
    out_A = f"{args.output_dir}/{args.sample}.{handle_A}.het_blocks.A.bed"
    out_B = f"{args.output_dir}/{args.sample}.{handle_B}.het_blocks.B.bed"
    Het_blocks_A.saveas(out_A)
    Het_blocks_B.saveas(out_B)
    sys.stderr.write(f"[{at()}] Parent A: Found {len(Het_blocks_A)} het blocks, {len(Homo_blocks_REF_A)} homo REF blocks, {len(Homo_blocks_ALT_A)} homo ALT blocks\n")
    sys.stderr.write(f"[{at()}] Parent B: Found {len(Het_blocks_B)} het blocks, {len(Homo_blocks_REF_B)} homo REF blocks, {len(Homo_blocks_ALT_B)} homo ALT blocks\n")

    # remove overlaps with heteroblocks from homoblocks
    sys.stderr.write(f"[{at()}] Trimming overlaps of homozygous blocks with any heterozygous block...\n")
    Homo_blocks_A = adjust_homo_coordinates_by_hetero_overlap(args, Het_blocks_A, Homo_blocks_REF_A, Homo_blocks_ALT_A)
    Homo_blocks_B = adjust_homo_coordinates_by_hetero_overlap(args, Het_blocks_B, Homo_blocks_REF_B, Homo_blocks_ALT_B)
    sys.stderr.write(f"[{at()}] Parent A: {len(Homo_blocks_A)} blocks were trimmed\n")
    sys.stderr.write(f"[{at()}] Parent B: {len(Homo_blocks_B)} blocks were trimmed\n")

    # remove uncovered regions
    sys.stderr.write(f"[{at()}] Trimming overlaps of homo-blocks with uncovered regions...\n")
    Homo_blocks_adj_A = remove_uncovered_regions(args, Chrom_bams_A, args.refs[0], Homo_blocks_A, genome_file_A, tmpdir)
    Homo_blocks_adj_B = remove_uncovered_regions(args, Chrom_bams_B, args.refs[1], Homo_blocks_B, genome_file_B, tmpdir)
    sys.stderr.write(f"[{at()}] Parent A: {len(Homo_blocks_adj_A)} blocks were trimmed\n")
    sys.stderr.write(f"[{at()}] Parent B: {len(Homo_blocks_adj_B)} blocks were trimmed\n")

    # analyse block coverage
    sys.stderr.write(f"[{at()}] Computing coverage up- , in-, and downstream of any candidate block...\n")
    LOH_A = assess_coverage(args, Chrom_bams_A, args.refs[0], Homo_blocks_A, handle_A)
    LOH_B = assess_coverage(args, Chrom_bams_B, args.refs[1], Homo_blocks_B, handle_B)
    sys.stderr.write(f"[{at()}] Parent A: {len(LOH_A)} blocks were processed\n")
    sys.stderr.write(f"[{at()}] Parent B: {len(LOH_B)} blocks were processed\n")

    # filter by length
    sys.stderr.write(f"[{at()}] Filtering candidate blocks by length...\n")
    LOH_A = filter_by_length(LOH_A, args.min_length)
    LOH_B = filter_by_length(LOH_B, args.min_length)
    sys.stderr.write(f"[{at()}] Parent A: {len(LOH_A)} blocks survived the filtering\n")
    sys.stderr.write(f"[{at()}] Parent B: {len(LOH_B)} blocks survived the filtering\n")

    # count homozygous SNPs in intervals
    sys.stderr.write(f"[{at()}] Adding heterozygous and homozygous SNP counts in each block...\n")
    LOH_A = count_snps_in_bed_interval(LOH_A, Vcfs["homo"][0])
    LOH_B = count_snps_in_bed_interval(LOH_B, Vcfs["homo"][1])

    # count heterozygous SNPs in intervals
    LOH_A = count_snps_in_bed_interval(LOH_A, Vcfs["hetero"][0])
    LOH_B = count_snps_in_bed_interval(LOH_B, Vcfs["hetero"][1])

    # sort
    LOH_A = sort_bed_intervals(LOH_A)
    LOH_B = sort_bed_intervals(LOH_B)

    out_A = open(f"{args.output_dir}/{args.sample}.LOH_candidates.A.tsv", "w")
    out_B = open(f"{args.output_dir}/{args.sample}.LOH_candidates.B.tsv", "w")
    out_A.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")
    out_B.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")

    for row in LOH_A:
        item = str(row).rstrip("\b\n\r").split("\t")
        item[1] = str(int(item[1])+1)
        out_A.write("\t".join(item) + "\n")

    for row in LOH_B:
        item = str(row).rstrip("\b\n\r").split("\t")
        item[1] = str(int(item[1])+1)
        out_B.write("\t".join(item) + "\n")

    out_A.close()
    out_B.close()

    sys.stderr.write(f"[{at()}] Parent A: {len(LOH_A)} blocks were processed\n")
    sys.stderr.write(f"[{at()}] Parent B: {len(LOH_B)} blocks were processed\n")

    # write candidates to output, pre filtering
    LOH_candidates_A = BedTool([str(row).rstrip("\b\n\r").split("\t")[0:3] for row in LOH_A])
    LOH_candidates_B = BedTool([str(row).rstrip("\b\n\r").split("\t")[0:3] for row in LOH_B])
    LOH_candidates_A.saveas(f"{args.output_dir}/{args.sample}.LOH_candidates.A.bed")
    LOH_candidates_B.saveas(f"{args.output_dir}/{args.sample}.LOH_candidates.B.bed")

    # keep only intervals inside --regions
    if args.regions:
        sys.stderr.write(f"[{at()}] Removing intervals outside of --regions...\n")
        LOH_A = keep_relevant_blocks(args, LOH_A)
        LOH_B = keep_relevant_blocks(args, LOH_B)

    sys.stderr.write(f"[{at()}] {len(LOH_A)} retained for parent A\n")
    sys.stderr.write(f"[{at()}] {len(LOH_B)} retained for parent B\n")

    # write to output
    # including a header
    sys.stderr.write(f"[{at()}] Writing to output...\n")
    out_A = f"{args.output_dir}/{args.sample}.LOH_blocks.A.tsv"
    out_B = f"{args.output_dir}/{args.sample}.LOH_blocks.B.tsv"
    out_A_bed = f"{args.output_dir}/{args.sample}.LOH_blocks.A.bed"
    out_B_bed = f"{args.output_dir}/{args.sample}.LOH_blocks.B.bed"
    OUTPUT_A = open(out_A, "w")
    OUTPUT_A_BED = open(out_A_bed, "w")
    OUTPUT_B = open(out_B, "w")
    OUTPUT_B_BED = open(out_B_bed, "w")
    OUTPUT_A.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")
    OUTPUT_B.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")

    for row in LOH_A:
        row_1based = str(row).rstrip("\b\n\r").split("\t")
        row_1based[1] = int(row_1based[1])+1
        row_1based = "\t".join([str(i) for i in row_1based]) + "\n"
        OUTPUT_A.write(str(row_1based))
        row = list(row)[0:3]
        OUTPUT_A_BED.write("\t".join(row) + "\n")
    OUTPUT_A.close()
    OUTPUT_A_BED.close()

    for row in LOH_B:
        row_1based = str(row).rstrip("\b\n\r").split("\t")
        row_1based[1] = int(row_1based[1])+1
        row_1based = "\t".join([str(i) for i in row_1based]) + "\n"
        OUTPUT_B.write(str(row_1based))
        row = list(row)[0:3]
        OUTPUT_B_BED.write("\t".join(row) + "\n")
    OUTPUT_B.close()
    OUTPUT_B_BED.close()

    sys.stderr.write(f"[{at()}] Done!\n")


def main(args, tmp_bams):

    """
    Last update: 28/09/2022
    """

    # check conditions before starting
    sys.stderr.write(f"[{at()}] Preparing workspace...\n")
    check_conditions(args)
    organize_workspace(args)

    # run in the selected mode
    if args.assign_blocks:
        sys.stderr.write(f"[{at()}] Running in block assignment mode...\n")
        run_in_block_assignment_mode(args, tmp_bams)

    else:
        sys.stderr.write(f"[{at()}] Running in default mode...\n")
        run_in_default_mode(args, tmp_bams)


if __name__ == "__main__":

    # use the systems tmpdir or create one in outdir
    if args.os_scratch:
        unique_key = generate_unique_key(os.environ["TMPDIR"])
        tmpdir = f"{os.environ['TMPDIR']}/tmp_{unique_key}"
        print(tmpdir)
    else:
        unique_key = generate_unique_key(args.output_dir)
        tmpdir = f"{args.output_dir}/tmp_{unique_key}"
    os.makedirs(tmpdir, exist_ok=True)
    os.chmod(tmpdir, 0o777)
    pybedtools.helpers.set_tempdir(tmpdir)

    tmp_bams = f"{tmpdir}/tmp_bams"
    os.makedirs(tmp_bams, exist_ok=True)

    main(args, tmp_bams)

    # remove tmp folder
    shutil.rmtree(tmpdir)
